Id,Question,Type,Preformatted,Image,Option 1,Image 1,Option 2,Image 2,Option 3,Image 3,Option 4,Image 4,Option 5,Image 5,Option 6,Image 6,Option 7,Image 7,Option 8,Image 8,Option 9,Image 9,Option 10,Image 10,Option 11,Image 11,Option 12,Image 12,Option 13,Image 13,Option 14,Image 14,Option 15,Image 15,Correct Answer
1,"Which of the following accurately describes the relationship between Reinforcement Learning, Machine Learning, and Artificial Intelligence?",MC,,,"Reinforcement Learning is a subfield of Machine Learning, which is a subfield of Artificial Intelligence.",,"Reinforcement Learning is a subfield of Artificial Intelligence, which is a subfield of Machine Learning.",,Reinforcement Learning and Machine Learning are both unrelated subfields of Artificial Intelligence.,,Reinforcement Learning and Artificial Intelligence are both unrelated subfields of Machine Learning.,,,,,,,,,,,,,,,,,,,,,,,,1
2,"Suppose that you have two reinforcement learning agents attempting to learn the optimal policy when acting in a <i>k</i>-armed bandit environment, both of which employ the \(\epsilon\)-greedy method for action selection. Both agents use values of \(\epsilon\) greater than 0 but less than 0.2, but one (labelled ""High Epsilon Value"") uses a higher value of \(\epsilon\) than the other (labelled ""Low Epsilon Value""). Which of the following plots most closely reflects the reward that would be earned by each agent over time?",MC,,,a,Quiz Q1-1.jpg,b,Quiz Q1-2.jpg,c,Quiz Q1-3.jpg,d,Quiz Q1-4.jpg,,,,,,,,,,,,,,,,,,,,,,,4
3,Suppose that an agent selects an action by computing a probability for each action based on that action's value and selecting an action using that probability distribution. The agent would be using a(n) _____ method of action selection.,MC,,,soft-max,,upper confidence bound,,greedy,,\(\epsilon\)-greedy,,,,,,,,,,,,,,,,,,,,,,,,1
4,"When Markov Decision Processes (MDPs) are used to model environments in reinforcement learning problems, one often assumes the Markov property, which is the property that any rewards or state transitions that occurred as a result of selecting an action while in a state at a particular time step is completely independent of what happened at any previous time step. This is based on the assumption that anything significant that happened at previous time steps would be encoded in the current _____ of the MDP.",MC,,,state.,,reward.,,state value.,,action value.,,,,,,,,,,,,,,,,,,,,,,,,1
5,Reinforcement learning agents employing Dynamic Programming methods compute optimal policies by...,MC,,,iterating back-and-forth between policy evaluation and policy execution.,,iterating back-and-forth between policy evaluation and policy improvement.,,using computed optimal policies from subsets of the Markov Decision Process.,,performing an exhaustive search through the space of policies and performing policy evaluation on all of them.,,,,,,,,,,,,,,,,,,,,,,,,2
6,Importance sampling is a useful technique that attempts to offset problems that could otherwise occur when using _____ methods.,MC,,,deep reinforcement learning,,planning,,on-policy,,off-policy,,,,,,,,,,,,,,,,,,,,,,,,4
7,"In the cliff walking example, an RL agent employing _____ is more likely to find the optimal policy, while an agent employing _____ is more likely to accumulate more reward over the course of its learning process. This is in part because _____ is an on-policy method while _____ is an off-policy method.",MC,,,"Sarsa, Q-Learning, Sarsa, Q-Learning",,"Sarsa, Q-Learning, Q-Learning, Sarsa",,"Q-Learning, Sarsa, Sarsa, Q-Learning",,"Q-Learning, Sarsa, Q-Learning, Sarsa",,,,,,,,,,,,,,,,,,,,,,,,3
8,"The Curriculum Learning algorithm described in <i>Source Task Creation for Curriculum Learning</i> by Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, and Peter Stone, is composed of a sequence of multiple instances of...",MC,,,multi-agent reinforcment learning.,,transfer learning.,,one-step temporal difference learning.,,rollout methods.,,,,,,,,,,,,,,,,,,,,,,,,2
9,"Consider Table 1, a Q-Table of action values, and Table 2, a table showing the steps taken and transitions in an episode. Suppose that an agent is learning using the 2-step Sarsa method with undiscounted returns and a learning rate (\(\alpha\)) of 0.5. Table 1 is currently reflective of the state of the Q-Table after <em>exactly one</em> update has been made this episode. The next Q-Table update is to the value \(Q(0, Right)\), which is updated to...
Table 1:
<table><tr><td><strong>State</strong></td><td><strong>Action</strong></td><td><strong>Q-Value</strong></td></tr><tr><td>0</td><td>Left</td><td>2</td></tr><tr><td>0</td><td>Right</td><td>4</td></tr><tr><td>1</td><td>Left</td><td>2</td></tr><tr><td>1</td><td>Right</td><td>8</td></tr><tr><td>2 (terminal)</td><td>Left</td><td>0</td></tr><tr><td>2 (terminal)</td><td>Right</td><td>0</td></tr></table>
Table 2:
<table><tr><td><strong>Time Step (\(t\))</strong></td><td><strong>State (\(S_t\))</strong></td><td><strong>Action (\(A_t\))</strong></td><td><strong>Reward (\(R_{t+1}\))</strong></td><td><strong>Next State (\(S_{t+1}\))</strong></td></tr><tr><td>0</td><td>0</td><td>Left</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>Right</td><td>2</td><td>1</td></tr><tr><td>2</td><td>1</td><td>Left</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>Right</td><td>2</td><td>1</td></tr><tr><td>4</td><td>1</td><td>Right</td><td>8</td><td>2</td></tr></table>",MC,,,2,,4,,5,,6,,,,,,,,,,,,,,,,,,,,,,,,3
10,Monte Carlo methods...,MC,,,both sample and bootstrap.,,sample but do not bootstrap.,,bootstrap but do not sample.,,neither sample nor bootstrap.,,,,,,,,,,,,,,,,,,,,,,,,2
11,Suppose that an agent is using the linear function approximation method using \(n\) features in a problem with \(N\) states. Updating one weight \(w_i\) may change up to _____ state value(s).,MC,,,\(1\),,\(n\),,\(\frac{N}{n}\),,\(N\),,,,,,,,,,,,,,,,,,,,,,,,4
12,Which of the following methods that may be used in function approximation is designed to handle continuous state spaces?,MC,,,Tile Coding,,Experience Replay,,Fourier Basis,,Stochastic Gradient Descent,,,,,,,,,,,,,,,,,,,,,,,,1
13,Which of the following statements is true regarding the use of eligibility traces in reinforcement learning?,MC,,,"The eligilibity of each state-action pair \(e(s, a)\) is reset to 0 between episodes.",,"At each episode time step, values of <em>all</em> state-action pairs need to be updated, not just the one visited.",,All of the above,,None of the above,,,,,,,,,,,,,,,,,,,,,,,,3
14,Which aspect of Genetic Algorithms is analogous to exploration in Reinforcement Learning algorithms?,MC,,,Recombination,,Selection,,The genome,,The fitness function,,,,,,,,,,,,,,,,,,,,,,,,1
15,With which of the following can an optimal stochastic policy be learned directly?,MC,,,Semi-gradient methods,,\(\lambda\)-return methods,,Policy gradient methods,,Both the first option and the second option.,,,,,,,,,,,,,,,,,,,,,,,,3
