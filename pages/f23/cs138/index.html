<!DOCTYPE html>
<!-- This HTML is pretty unreadable because it was generated. Either copy and paste this into a beautifier or see a human written example in /pages/nonAc/pokegeo/example.html. -->
<html><head><title>Randy's Review Questions</title><link rel="stylesheet" href="../../../css/style.css" type="text/css" /><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></head><h1>Randy's Review Questions</h1><h2>CS 138: Reinforcement Learning</h2><p>Based on content taught by Jivko Sinapov in Fall 2023.</p><p>You are currently in learn mode. <a href="test.html">To switch to test mode, click here.</a></p><hr /><p class="question">1. Which of the following accurately describes the relationship between Reinforcement Learning, Machine Learning, and Artificial Intelligence?</p><ul><input type="radio" name="q1" value="1" />Reinforcement Learning is a subfield of Machine Learning, which is a subfield of Artificial Intelligence.<br /><input type="radio" name="q1" value="2" />Reinforcement Learning is a subfield of Artificial Intelligence, which is a subfield of Machine Learning.<br /><input type="radio" name="q1" value="3" />Reinforcement Learning and Machine Learning are both unrelated subfields of Artificial Intelligence.<br /><input type="radio" name="q1" value="4" />Reinforcement Learning and Artificial Intelligence are both unrelated subfields of Machine Learning.<br /></ul><input type="button" onclick="checkquestionans(0)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q1correct" style="color:green;display:none;">Correct!</p><p id="q1incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">2. Suppose that you have two reinforcement learning agents attempting to learn the optimal policy when acting in a <i>k</i>-armed bandit environment, both of which employ the \(\epsilon\)-greedy method for action selection. Both agents use values of \(\epsilon\) greater than 0 but less than 0.2, but one (labelled "High Epsilon Value") uses a higher value of \(\epsilon\) than the other (labelled "Low Epsilon Value"). Which of the following plots most closely reflects the reward that would be earned by each agent over time?</p><ul><input type="radio" name="q2" value="1" />a<img src="Quiz Q1-1.jpg" alt="Quiz Q1-1.jpg" /><br /><input type="radio" name="q2" value="2" />b<img src="Quiz Q1-2.jpg" alt="Quiz Q1-2.jpg" /><br /><input type="radio" name="q2" value="3" />c<img src="Quiz Q1-3.jpg" alt="Quiz Q1-3.jpg" /><br /><input type="radio" name="q2" value="4" />d<img src="Quiz Q1-4.jpg" alt="Quiz Q1-4.jpg" /><br /></ul><input type="button" onclick="checkquestionans(1)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q2correct" style="color:green;display:none;">Correct!</p><p id="q2incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">3. Suppose that an agent selects an action by computing a probability for each action based on that action's value and selecting an action using that probability distribution. The agent would be using a(n) _____ method of action selection.</p><ul><input type="radio" name="q3" value="1" />soft-max<br /><input type="radio" name="q3" value="2" />upper confidence bound<br /><input type="radio" name="q3" value="3" />greedy<br /><input type="radio" name="q3" value="4" />\(\epsilon\)-greedy<br /></ul><input type="button" onclick="checkquestionans(2)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q3correct" style="color:green;display:none;">Correct!</p><p id="q3incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">4. When Markov Decision Processes (MDPs) are used to model environments in reinforcement learning problems, one often assumes the Markov property, which is the property that any rewards or state transitions that occurred as a result of selecting an action while in a state at a particular time step is completely independent of what happened at any previous time step. This is based on the assumption that anything significant that happened at previous time steps would be encoded in the current _____ of the MDP.</p><ul><input type="radio" name="q4" value="1" />state.<br /><input type="radio" name="q4" value="2" />reward.<br /><input type="radio" name="q4" value="3" />state value.<br /><input type="radio" name="q4" value="4" />action value.<br /></ul><input type="button" onclick="checkquestionans(3)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q4correct" style="color:green;display:none;">Correct!</p><p id="q4incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">5. Reinforcement learning agents employing Dynamic Programming methods compute optimal policies by...</p><ul><input type="radio" name="q5" value="1" />iterating back-and-forth between policy evaluation and policy execution.<br /><input type="radio" name="q5" value="2" />iterating back-and-forth between policy evaluation and policy improvement.<br /><input type="radio" name="q5" value="3" />using computed optimal policies from subsets of the Markov Decision Process.<br /><input type="radio" name="q5" value="4" />performing an exhaustive search through the space of policies and performing policy evaluation on all of them.<br /></ul><input type="button" onclick="checkquestionans(4)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q5correct" style="color:green;display:none;">Correct!</p><p id="q5incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">6. Importance sampling is a useful technique that attempts to offset problems that could otherwise occur when using _____ methods.</p><ul><input type="radio" name="q6" value="1" />deep reinforcement learning<br /><input type="radio" name="q6" value="2" />planning<br /><input type="radio" name="q6" value="3" />on-policy<br /><input type="radio" name="q6" value="4" />off-policy<br /></ul><input type="button" onclick="checkquestionans(5)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q6correct" style="color:green;display:none;">Correct!</p><p id="q6incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">7. In the cliff walking example, an RL agent employing _____ is more likely to find the optimal policy, while an agent employing _____ is more likely to accumulate more reward over the course of its learning process. This is in part because _____ is an on-policy method while _____ is an off-policy method.</p><ul><input type="radio" name="q7" value="1" />Sarsa, Q-Learning, Sarsa, Q-Learning<br /><input type="radio" name="q7" value="2" />Sarsa, Q-Learning, Q-Learning, Sarsa<br /><input type="radio" name="q7" value="3" />Q-Learning, Sarsa, Sarsa, Q-Learning<br /><input type="radio" name="q7" value="4" />Q-Learning, Sarsa, Q-Learning, Sarsa<br /></ul><input type="button" onclick="checkquestionans(6)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q7correct" style="color:green;display:none;">Correct!</p><p id="q7incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">8. The Curriculum Learning algorithm described in <i>Source Task Creation for Curriculum Learning</i> by Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, and Peter Stone, is composed of a sequence of multiple instances of...</p><ul><input type="radio" name="q8" value="1" />multi-agent reinforcment learning.<br /><input type="radio" name="q8" value="2" />transfer learning.<br /><input type="radio" name="q8" value="3" />one-step temporal difference learning.<br /><input type="radio" name="q8" value="4" />rollout methods.<br /></ul><input type="button" onclick="checkquestionans(7)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q8correct" style="color:green;display:none;">Correct!</p><p id="q8incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">9. Consider Table 1, a Q-Table of action values, and Table 2, a table showing the steps taken and transitions in an episode. Suppose that an agent is learning using the 2-step Sarsa method with undiscounted returns and a learning rate (\(\alpha\)) of 0.5. Table 1 is currently reflective of the state of the Q-Table after <em>exactly one</em> update has been made this episode. The next Q-Table update is to the value \(Q(0, Right)\), which is updated to...</p><p class="question">Table 1:</p><p class="question"><table><tr><td><strong>State</strong></td><td><strong>Action</strong></td><td><strong>Q-Value</strong></td></tr><tr><td>0</td><td>Left</td><td>2</td></tr><tr><td>0</td><td>Right</td><td>4</td></tr><tr><td>1</td><td>Left</td><td>2</td></tr><tr><td>1</td><td>Right</td><td>8</td></tr><tr><td>2 (terminal)</td><td>Left</td><td>0</td></tr><tr><td>2 (terminal)</td><td>Right</td><td>0</td></tr></table></p><p class="question">Table 2:</p><p class="question"><table><tr><td><strong>Time Step (\(t\))</strong></td><td><strong>State (\(S_t\))</strong></td><td><strong>Action (\(A_t\))</strong></td><td><strong>Reward (\(R_{t+1}\))</strong></td><td><strong>Next State (\(S_{t+1}\))</strong></td></tr><tr><td>0</td><td>0</td><td>Left</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>Right</td><td>2</td><td>1</td></tr><tr><td>2</td><td>1</td><td>Left</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>Right</td><td>2</td><td>1</td></tr><tr><td>4</td><td>1</td><td>Right</td><td>8</td><td>2</td></tr></table></p><ul><input type="radio" name="q9" value="1" />2<br /><input type="radio" name="q9" value="2" />4<br /><input type="radio" name="q9" value="3" />5<br /><input type="radio" name="q9" value="4" />6<br /></ul><input type="button" onclick="checkquestionans(8)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q9correct" style="color:green;display:none;">Correct!</p><p id="q9incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">10. Monte Carlo methods...</p><ul><input type="radio" name="q10" value="1" />both sample and bootstrap.<br /><input type="radio" name="q10" value="2" />sample but do not bootstrap.<br /><input type="radio" name="q10" value="3" />bootstrap but do not sample.<br /><input type="radio" name="q10" value="4" />neither sample nor bootstrap.<br /></ul><input type="button" onclick="checkquestionans(9)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q10correct" style="color:green;display:none;">Correct!</p><p id="q10incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">11. Suppose that an agent is using the linear function approximation method using \(n\) features in a problem with \(N\) states. Updating one weight \(w_i\) may change up to _____ state value(s).</p><ul><input type="radio" name="q11" value="1" />\(1\)<br /><input type="radio" name="q11" value="2" />\(n\)<br /><input type="radio" name="q11" value="3" />\(\frac{N}{n}\)<br /><input type="radio" name="q11" value="4" />\(N\)<br /></ul><input type="button" onclick="checkquestionans(10)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q11correct" style="color:green;display:none;">Correct!</p><p id="q11incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">12. Which of the following methods that may be used in function approximation is designed to handle continuous state spaces?</p><ul><input type="radio" name="q12" value="1" />Tile Coding<br /><input type="radio" name="q12" value="2" />Experience Replay<br /><input type="radio" name="q12" value="3" />Fourier Basis<br /><input type="radio" name="q12" value="4" />Stochastic Gradient Descent<br /></ul><input type="button" onclick="checkquestionans(11)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q12correct" style="color:green;display:none;">Correct!</p><p id="q12incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">13. Which of the following statements is true regarding the use of eligibility traces in reinforcement learning?</p><ul><input type="radio" name="q13" value="1" />The eligilibity of each state-action pair \(e(s, a)\) is reset to 0 between episodes.<br /><input type="radio" name="q13" value="2" />At each episode time step, values of <em>all</em> state-action pairs need to be updated, not just the one visited.<br /><input type="radio" name="q13" value="3" />All of the above<br /><input type="radio" name="q13" value="4" />None of the above<br /></ul><input type="button" onclick="checkquestionans(12)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q13correct" style="color:green;display:none;">Correct!</p><p id="q13incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">14. Which aspect of Genetic Algorithms is analogous to exploration in Reinforcement Learning algorithms?</p><ul><input type="radio" name="q14" value="1" />Recombination<br /><input type="radio" name="q14" value="2" />Selection<br /><input type="radio" name="q14" value="3" />The genome<br /><input type="radio" name="q14" value="4" />The fitness function<br /></ul><input type="button" onclick="checkquestionans(13)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q14correct" style="color:green;display:none;">Correct!</p><p id="q14incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><p class="question">15. With which of the following can an optimal stochastic policy be learned directly?</p><ul><input type="radio" name="q15" value="1" />Semi-gradient methods<br /><input type="radio" name="q15" value="2" />\(\lambda\)-return methods<br /><input type="radio" name="q15" value="3" />Policy gradient methods<br /><input type="radio" name="q15" value="4" />Both the first option and the second option.<br /></ul><input type="button" onclick="checkquestionans(14)" id="single_question" class="mybutton" value="Check Answer" /><br /><p id="q15correct" style="color:green;display:none;">Correct!</p><p id="q15incorrect" style="display:none;"><span style="color:purple;">Incorrect.</span> Try again.</p><br /><hr /><a href="../../../index.html">Return to Home</a><script src="../../../scripts/checkans.js"></script><script>function checkquestionans(number) { answerKey = [["1"], ["4"], ["1"], ["1"], ["2"], ["4"], ["3"], ["2"], ["3"], ["2"], ["4"], ["1"], ["3"], ["1"], ["3"]]; checksingleans(number, answerKey, false); }</script><noscript><p>Looks like JavaScript is disabled! The answer checker will not work without JavaScript enabled.</p></noscript></html>